---
sidebar_label: 'Artikel 5'
sidebar_position: 2
---


## Verboden AI-praktijken

1.   De volgende AI-praktijken zijn verboden:

    a) het in de handel brengen, het in gebruik stellen of het gebruiken van een AI-systeem dat subliminale technieken waarvan personen zich niet bewust zijn of doelbewust manipulatieve of misleidende technieken gebruikt, met als doel of effect het gedrag van personen of een groep personen wezenlijk te verstoren door hun vermogen om een geïnformeerd besluit te nemen merkbaar te belemmeren, waardoor zij een besluit nemen dat zij anders niet hadden genomen, op een wijze die ertoe leidt of er redelijkerwijs waarschijnlijk toe zal leiden dat deze of andere personen, of een groep personen, aanzienlijke schade oplopen;

<HighlightPopup 
  color="#DDA455" 
  type="text"
  content={<p>Examples of subliminal techniques (not necessarily prohibited unless all other conditions listed in Article 5(1)(a) AI Act are fulfilled) include:<br /> <br />
<strong>Visual Subliminal Messages:</strong> an AI system may show or embed images or text flashed briefly during video playback which are technically visible, but flashed too quickly for the conscious mind to register, while still being capable of influencing attitudes or behaviours.<br /> <br /> 
<strong>Auditory Subliminal Messages:</strong> an AI system may deploy sounds or verbal messages at low volumes or masked by other sounds, influencing the listener without conscious awareness. These sounds are still technically within the range of hearing, but are not consciously noticed by the listener due to their subtlety or masking by other audio.<br /><br />  <strong>Tactile Subliminal Stimuli:</strong> an AI system may stimulate subtle physical sensations that are perceived unconsciously, capable of influencing emotional states or behaviour.<br /><br />  
<strong>Subvisual and Subaudible Cueing:</strong> an AI system may deploy stimuli that are not just subtle or masked, but are presented in a way that makes them entirely undetectable by the human senses under normal conditions, for example flashing visual stimuli (e.g. flashing images) too quickly for the human eye to detect consciously or playing sounds at volumes imperceptible to the human ear.<br /><br />  
<strong>Embedded Images:</strong> an AI system may hide images within other visual content which are not consciously perceived, but may still be processed by the brain and influence behaviour.  
</p>}
> Guideline </HighlightPopup>

<HighlightPopup 
  color="#217A7D" 
  type="text"
  content={<p>Overweging 29:<br /><br /> Op AI gebaseerde manipulatietechnieken kunnen worden gebruikt om gebruikers te overtuigen zich ongewenst te gedragen en ze te misleiden door hen aan te zetten tot bepaalde beslissingen waardoor hun autonomie, besluitvorming en keuze worden ondermijnd en beperkt. Het in de handel brengen, in gebruik stellen of gebruiken van bepaalde AI-systemen met als doel of gevolg het menselijk gedrag wezenlijk te verstoren, waarbij waarschijnlijk aanzienlijke schade, met name met voldoende belangrijke negatieve gevolgen voor de fysieke of psychologische gezondheid of voor financiële belangen, zal optreden, is bijzonder gevaarlijk en moet daarom worden verboden. Dergelijke AI-systemen maken gebruik van subliminale componenten zoals audio-, beeld- en videostimuli die personen niet kunnen waarnemen, omdat deze stimuli verder gaan dan de menselijke perceptie, of andere manipulatieve of bedrieglijke technieken die de autonomie, besluitvorming of vrije keuze van personen ondermijnen of beperken op wijzen waarvan mensen zich van deze technieken niet bewust zijn of waarbij, zelfs als ze zich er bewust van zijn, ze nog steeds misleid kunnen worden of hen niet kunnen beheersen of weerstaan. Dit kan bijvoorbeeld worden vergemakkelijkt door machine-hersen-interfaces of virtuele realiteit, aangezien zij een hogere mate van controle mogelijk maken over de stimuli die aan personen worden gepresenteerd, voor zover zij hun gedrag wezenlijk kunnen verstoren op een aanzienlijk schadelijke manier.<br /> <br />  Daarnaast kunnen AI-systemen ook kwetsbaarheden uitbuiten van een persoon of een specifieke groep personen als gevolg van hun leeftijd, handicap in de zin van Richtlijn (EU) 2019/882 van het Europees Parlement en de Raad (16) of een specifieke sociale of economische situatie waardoor deze personen waarschijnlijk kwetsbaarder zullen zijn voor uitbuiting, zoals personen die in extreme armoede leven of personen die tot een etnische of religieuze minderheid behoren. Dergelijke AI-systemen kunnen in de handel worden gebracht, in gebruik worden gesteld of worden gebruikt met als doel of als gevolg dat het gedrag van een persoon wezenlijk wordt verstoord op een manier die deze persoon, een andere persoon of groepen personen aanzienlijke schade berokkent of redelijkerwijs dreigt te berokkenen, met inbegrip van schade die in de loop der tijd erger kan worden, en moeten daarom worden verboden. Er kan worden aangenomen dat er geen sprake is van een intentie om gedrag te verstoren wanneer de verstoring het gevolg is van factoren buiten het AI-systeem waarop de aanbieder of de gebruiksverantwoordelijke geen invloed heeft, namelijk factoren die redelijkerwijs niet te voorzien zijn en die de aanbieder of de gebruiksverantwoordelijke van het AI-systeem bijgevolg niet kan beperken. In ieder geval is het niet vereist dat de aanbieder of de gebruiksverantwoordelijke de intentie heeft aanzienlijke schade te berokkenen, op voorwaarde dat deze schade het gevolg is van op AI gebaseerde manipulatieve of uitbuitingspraktijken. De verbodsbepalingen voor dergelijke AI-praktijken vormen een aanvulling op de bepalingen van Richtlijn 2005/29/EG van het Europees Parlement en de Raad (17), met name die bepalingen waarin is vastgesteld dat oneerlijke handelspraktijken die tot economische of financiële schade voor consumenten leiden onder alle omstandigheden verboden zijn, ongeacht of zij via AI-systemen of anderszins tot stand worden gebracht. Het verbod op manipulatie- en uitbuitingspraktijken in deze verordening mag geen afbreuk doen aan rechtmatige praktijken in het kader van medische behandelingen, zoals psychologische behandeling van een psychische aandoening of lichamelijke revalidatie, wanneer die praktijken worden uitgevoerd overeenkomstig de toepasselijke medische normen en wetgeving, bijvoorbeeld met uitdrukkelijke toestemming van de natuurlijke personen of hun wettelijke vertegenwoordigers. Bovendien mogen gangbare en legitieme handelspraktijken, bijvoorbeeld in de reclamesector, die in overeenstemming zijn met het toepasselijke recht op zich niet worden beschouwd als schadelijke manipulatieve op AI gebaseerde praktijken.  
</p>}
> Overweging 29 </HighlightPopup>

<HighlightPopup 
  color="#25c2a0" 
  type="iframe"
  content="https://digitaldecadewebsite.vercel.app/docs/Wetgevingen/DSA/Hoofdstuk%20I%20-%20Algemene%20bepalingen/Artikel%201"
> Zie ook artikel 25 DSA </HighlightPopup>

import Arnoud from '@site/src/components/Arnoud';

<Arnoud popupText="The first prohibited practice deals with so-called subliminal manipulation. Generally, subliminal techniques aim to influence individuals using stimuli below an individual’s perception thresholds, while manipulation is distorting the form or structure of the judgment process, leading to outcomes that may not be in the best interests of the decision maker.  A comparable criterion is that of the unfair commercial practice Directive (2005/29, article 5(2)(a)) which requires that the message “is likely to materially distort the economic behaviour” of the consumer.
Subliminal messaging. Subliminal messaging has been the subject of controversy since 1957, when researcher James Vicary hid the message “Hungry? Eat popcorn!” in a movie shown in a New Jersey movie theatre and claimed it increased popcorn sales by 57%. No study ever showed any significant effect of subliminal advertising. The prohibition has been criticized as superfluous, as it overlaps with other bans, notably article 9(1)(b) of the Audiovisual Media Services Directive (2010/13) and article 25(1) of the Digital Services Act (2022/2065).
Machine-brain interfaces. Typically, subliminal messaging is realised through audiovisual output such as text, images or movies. Recital 29 explains that subliminal messaging involves “subliminal components such as audio, image, video stimuli that persons cannot perceive as those stimuli are beyond human perception or other manipulative or deceptive techniques that subvert or impair person’s autonomy, decision - making or free choices in ways that people are not consciously aware of, or even if aware they are still deceived or not able to control or resist.” This is of particular concern when machine-brain interfaces or virtual reality is employed.
Medical treatment. The prohibition does not extend to lawful practices in the context of medical treatment such as psychological treatment of a mental disease or physical rehabilitation, when those practices are carried out in accordance with the applicable legislation and medical standards, for example explicit consent of the individuals or their legal representatives (recital 29)." />

<br /> 


    b) het in de handel brengen, het in gebruik stellen of het gebruiken van een AI-systeem dat gebruikmaakt van de kwetsbaarheden van een natuurlijke persoon of specifieke groep personen als gevolg van hun leeftijd, handicap of een specifieke sociale of economische omstandigheid, met als doel of met als gevolg het gedrag van die persoon of personen die tot deze groep behoren, wezenlijk te verstoren op een wijze die ertoe leidt of waarvan redelijkerwijze te verwachten is dat deze ertoe zal leiden dat deze of andere personen aanzienlijke schade oplopen; 

    <HighlightPopup 
  color="#DDA455" 
  type="text"
  content={<p>AI systems used for providing banking services, such as mortgages and loans, that use the age or the specific socio- economic situation of the client as an input, in compliance with Union legislation on financial service, consumer protection, data protection and non-discrimination, do not qualify as the exploitation of vulnerabilities within the meaning of Article 5(1)(b) AI Act when they are designed to protect and support people identified as vulnerable due to their age, disability or specific socio-economic circumstances and are beneficial for those groups, contributing also to fairer and more sustainable financial services for those groups. AI systems that detect drowsiness and fatigue in drivers and alert them to rest in compliance with safety laws are beneficial and do not qualify as exploitation of vulnerabilities within the meaning of Article 5(1)(b) AI Act.</p>}
> Guideline </HighlightPopup>

<HighlightPopup 
  color="#217A7D" 
  type="text"
  content={<p>Op AI gebaseerde manipulatietechnieken kunnen worden gebruikt om gebruikers te overtuigen zich ongewenst te gedragen en ze te misleiden door hen aan te zetten tot bepaalde beslissingen waardoor hun autonomie, besluitvorming en keuze worden ondermijnd en beperkt. Het in de handel brengen, in gebruik stellen of gebruiken van bepaalde AI-systemen met als doel of gevolg het menselijk gedrag wezenlijk te verstoren, waarbij waarschijnlijk aanzienlijke schade, met name met voldoende belangrijke negatieve gevolgen voor de fysieke of psychologische gezondheid of voor financiële belangen, zal optreden, is bijzonder gevaarlijk en moet daarom worden verboden. Dergelijke AI-systemen maken gebruik van subliminale componenten zoals audio-, beeld- en videostimuli die personen niet kunnen waarnemen, omdat deze stimuli verder gaan dan de menselijke perceptie, of andere manipulatieve of bedrieglijke technieken die de autonomie, besluitvorming of vrije keuze van personen ondermijnen of beperken op wijzen waarvan mensen zich van deze technieken niet bewust zijn of waarbij, zelfs als ze zich er bewust van zijn, ze nog steeds misleid kunnen worden of hen niet kunnen beheersen of weerstaan. Dit kan bijvoorbeeld worden vergemakkelijkt door machine-hersen-interfaces of virtuele realiteit, aangezien zij een hogere mate van controle mogelijk maken over de stimuli die aan personen worden gepresenteerd, voor zover zij hun gedrag wezenlijk kunnen verstoren op een aanzienlijk schadelijke manier.<br /> <br />  Daarnaast kunnen AI-systemen ook kwetsbaarheden uitbuiten van een persoon of een specifieke groep personen als gevolg van hun leeftijd, handicap in de zin van Richtlijn (EU) 2019/882 van het Europees Parlement en de Raad (16) of een specifieke sociale of economische situatie waardoor deze personen waarschijnlijk kwetsbaarder zullen zijn voor uitbuiting, zoals personen die in extreme armoede leven of personen die tot een etnische of religieuze minderheid behoren. Dergelijke AI-systemen kunnen in de handel worden gebracht, in gebruik worden gesteld of worden gebruikt met als doel of als gevolg dat het gedrag van een persoon wezenlijk wordt verstoord op een manier die deze persoon, een andere persoon of groepen personen aanzienlijke schade berokkent of redelijkerwijs dreigt te berokkenen, met inbegrip van schade die in de loop der tijd erger kan worden, en moeten daarom worden verboden. Er kan worden aangenomen dat er geen sprake is van een intentie om gedrag te verstoren wanneer de verstoring het gevolg is van factoren buiten het AI-systeem waarop de aanbieder of de gebruiksverantwoordelijke geen invloed heeft, namelijk factoren die redelijkerwijs niet te voorzien zijn en die de aanbieder of de gebruiksverantwoordelijke van het AI-systeem bijgevolg niet kan beperken. In ieder geval is het niet vereist dat de aanbieder of de gebruiksverantwoordelijke de intentie heeft aanzienlijke schade te berokkenen, op voorwaarde dat deze schade het gevolg is van op AI gebaseerde manipulatieve of uitbuitingspraktijken. De verbodsbepalingen voor dergelijke AI-praktijken vormen een aanvulling op de bepalingen van Richtlijn 2005/29/EG van het Europees Parlement en de Raad (17), met name die bepalingen waarin is vastgesteld dat oneerlijke handelspraktijken die tot economische of financiële schade voor consumenten leiden onder alle omstandigheden verboden zijn, ongeacht of zij via AI-systemen of anderszins tot stand worden gebracht. Het verbod op manipulatie- en uitbuitingspraktijken in deze verordening mag geen afbreuk doen aan rechtmatige praktijken in het kader van medische behandelingen, zoals psychologische behandeling van een psychische aandoening of lichamelijke revalidatie, wanneer die praktijken worden uitgevoerd overeenkomstig de toepasselijke medische normen en wetgeving, bijvoorbeeld met uitdrukkelijke toestemming van de natuurlijke personen of hun wettelijke vertegenwoordigers. Bovendien mogen gangbare en legitieme handelspraktijken, bijvoorbeeld in de reclamesector, die in overeenstemming zijn met het toepasselijke recht op zich niet worden beschouwd als schadelijke manipulatieve op AI gebaseerde praktijken."</p>}
> Overweging 29 </HighlightPopup>

<Arnoud popupText="Use of AI systems may exploit vulnerabilities of a person or a specific group of persons due to their age, disability within the meaning of the Accessibility Directive (2019/882), or a specific social or economic situation that is likely to make those persons more vulnerable to exploitation such as persons living in extreme poverty, ethnic or religious minorities. Generally on this topic is Leiser. 
Exploitation. The prohibition is specific to use of AI to exploit vulnerabilities. The fact that use of an AI system puts the vulnerable at a disadvantage (e.g. through algorithmic bias or the inability to challenge AI-driven decisions due to exceptional circumstances) does not trigger the prohibition. An example would be a social chatbot that identifies lonely elderly men and manipulates them into paying for all sorts of premium ‘friendship’ features with AI-generated female-presenting content.
Disability. The European Accessibility Act (2019/882) defines in article 3(1) “‘persons with disabilities” as persons who have long-term physical, mental, intellectual or sensory impairments which in interaction with various barriers may hinder their full and effective participation in society on an equal basis with others.
Specific social or economic situation. Persons or groups can become vulnerable o manipulation in a wide variety of situations, such as when living in extreme poverty, or being ethnic or religious minorities (recital 29). Vulnerabilities can be temporary, e.g. when undergoing grief, sorrow or emotional distress. It is unclear if the use of a specific persuasion profile (microtargeting) would qualify, as this does not specifically target a vulnerability but rather a specific set of circumstance.
External factors. The prohibition does not apply when the distortion results from factors external to the AI system which are outside of the control of the provider or the deployer, meaning factors that may not be reasonably foreseen and mitigated by the provider or the deployer of the AI system (recital 29). For instance, if a synthetic content marker (article 50) in AI-generated advertisements is not adequately picked up by a platform the content would be perceived as authentic, causing a distorted perception with viewers. 
No malicious intent. Recital 29 adds that it is not necessary for the provider or the deployer to have the intention to cause significant harm, as long as such harm results from the manipulative or exploitative AI-enabled practices. 
Unfair commercial practices. A comparable ban can be found in article 9(b) of the Unfair Commercial Practices Directive (2005/29), which declares an aggressive commercial practice “the [knowing] exploitation of any specific misfortune or circumstance of such gravity as to impair the consumer’s judgement to influence the consumer's decision with regard to the product”. The present prohibition is complementary to that Directive (recital 29), but “common and legitimate commercial practices, for example in the field of advertising, that are in compliance with the applicable law should not in themselves be regarded as constituting harmful manipulative AI practices.”
Medical treatment. The prohibition does not extend to lawful practices in the context of medical treatment such as psychological treatment of a mental disease or physical rehabilitation, when those practices are carried out in accordance with the applicable legislation and medical standards, for example explicit consent of the individuals or their legal representatives (recital 29).
" />

    c) het in de handel brengen, het in gebruik stellen of het gebruiken van AI-systemen voor de evaluatie of classificatie van natuurlijke personen of groepen personen gedurende een bepaalde periode op basis van hun sociale gedrag of bekende, afgeleide of voorspelde persoonlijke of persoonlijkheidskenmerken, waarbij de sociale score een of beide van de volgende gevolgen heeft:

        i) de nadelige of ongunstige behandeling van bepaalde natuurlijke personen of groepen personen in een sociale context die geen verband houdt met de context waarin de data oorspronkelijk werden gegenereerd of verzameld;

        ii) de nadelige of ongunstige behandeling van bepaalde natuurlijke personen of groepen personen die ongerechtvaardigd of onevenredig met hun sociale gedrag of de ernst hiervan is;

    <HighlightPopup 
  color="#DDA455" 
  type="text"
  content={<p>Examples of detrimental or unfavourable treatment in unrelated social contexts prohibited under Article 5(1)(c) i) AI Act <br /><br />National tax authorities use an AI predictive tool on all taxpayers’ tax returns in a country to select tax returns for closer inspection. The AI tool uses relevant variables, such as yearly income, assets (real estate property, cars etc.), data on family members of beneficiaries, but also unrelated data, such as taxpayers’ social habits or internet connections, to single out specific individuals for inspections.<br /><br />A social welfare agency uses an AI system to estimate the probability of fraud by beneficiaries of household allowances that relies on characteristics collected or inferred from social contexts with no apparent connection or relevance for the assessment of fraud, such as having a spouse of a certain nationality or ethnic origin, having an internet connection, behaviour on social platforms, or performance at the workplace, etc. 122 By contrast, data that is relevant for the allocation of the benefits and lawfully collected could be used to determine the risk of fraud, since public authorities pursue a legitimate aim in verifying if social benefits are correctly allocated.<br /><br /> A public labour agency uses an AI system to score unemployed individuals based on an interview and an AI-based assessment for determining whether an individual should benefit from state support for employment. That score is based on relevant personal characteristics, such as age and education, but also variables collected or inferred from data and contexts with no apparent connection to the purpose of evaluation, such as marital status, health data for chronic diseases, addiction, etc.<br /><br /> These unacceptable scoring practices may be distinguished from lawful practices that evaluate persons for specific purpose in compliance with Union and national law, in particular when such laws, in compliance with EU law, specify the data considered as relevant and necessary for the purposes of evaluation</p>}
> Guideline </HighlightPopup>

<HighlightPopup 
  color="#217A7D" 
  type="text"
  content={<p>AI-systemen die door publieke of door private actoren worden gebruikt om een beoordeling van natuurlijke personen (“social scoring”) uit te voeren, kunnen discriminerende resultaten en de uitsluiting van bepaalde groepen tot gevolg hebben. Deze systemen kunnen een schending inhouden van het recht op waardigheid en non-discriminatie en van waarden als gelijkheid en rechtvaardigheid. Dergelijke AI-systemen beoordelen of classificeren natuurlijke personen of groepen natuurlijke personen op basis van meerdere datapunten met betrekking tot hun sociale gedrag in meerdere contexten of op basis van bekende, afgeleide of voorspelde persoonlijke of persoonlijkheidskenmerken gedurende een bepaalde periode. De sociale score die dergelijke AI-systemen opleveren, kan leiden tot een nadelige of ongunstige behandeling van natuurlijke personen of hele groepen natuurlijke personen in sociale contexten die geen verband houden met de context waarin de data oorspronkelijk zijn gegenereerd of verzameld, of tot een nadelige behandeling die onevenredig of ongerechtvaardigd is in verhouding tot de ernst van het sociale gedrag. AI-systemen die dergelijke onaanvaardbare scoringpraktijken met zich meebrengen en tot dergelijke nadelige of ongunstige resultaten leiden, moeten daarom worden verboden. Dat verbod mag geen afbreuk doen aan wettige praktijken voor de evaluatie van natuurlijke personen die worden verricht voor een specifiek doeleinde in overeenstemming met het Unierecht en het nationale recht.</p>}
> Overweging 31 </HighlightPopup>

<Arnoud popupText="Social scoring is the practice of evaluating behaviour of persons (or groups) in multiple contexts over time, and assigning credits or demerits to observed behaviour or expressed characteristics. For instance, a person jaywalking or ignoring a red light would receive demerits, while a person assisting an infirm person crossing the road would receive credits. A too-low social score disqualifies the person from certain societal privileges, such as being allowed to ride public transportation. As recital 31 puts it succinctly, such systems “may violate the right to dignity and non-discrimination and the values of equality and justice.”
Unrelated contexts. The first criterion is that the consequence of the unwanted behaviour is meted out in an unrelated context. For instance, banning a student from a class after an AI system detects plagiarism in a paper would not be an unrelated context, but disinviting the student from the school’s graduation ball (such as the German Abiball or the Norwegian Nyttårsball) could be.
Unjustified or disproportionate. The second (and cumulative) criterion is that the consequence is unjustified or disproportionate given the offence. 
Over time. The third cumulative criterion is that the practice is conducted over time, as opposed to a one-time consequence for one particular offence.
Lawful evaluation. Recital 31 adds that this prohibition “should not affect lawful evaluation practices of natural persons done for a specific purpose in compliance with national and Union law.” It is therefore limited to large-scale and generic evaluations and assessments tied to credit/demerit systems.
China’s Social Credit project. This prohibition can be traced to the “social credit system project” (SCSP) unveiled in 2014 in the People’s Republic of China.  No actual state-wide implementation of the SCSP has arisen, but it has significantly pushed the debate on AI-driven mass surveillance and law enforcement.
" />


import React from 'react';
import Link from '@docusaurus/Link';


import HighlightPopup from '@site/src/components/HighlightPopup';





